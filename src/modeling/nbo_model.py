"""
–ú–æ–¥—É–ª—å –¥–ª—è –º–æ–¥–µ–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π Next Best Offer.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤.
"""

import joblib
from pathlib import Path
from typing import Dict, List, Optional
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

from src.features.user_profile import profile_to_features
from src.utils.yandex_gpt_client import call_yandex_gpt


class NBOModel:
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ Next Best Offer.
    """
    
    def __init__(self, model_path: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏.
        
        :param model_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        self.model_path = model_path or "models/nbo_model.pkl"
        self.model: Optional[RandomForestRegressor] = None
        self.scaler: Optional[StandardScaler] = None
        self.products: List[str] = []
        
        if Path(self.model_path).exists():
            self.load_model()
        else:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
            self.model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
            self.scaler = StandardScaler()
            self.products = ["–ò–ø–æ—Ç–µ–∫–∞", "–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", "–í–∫–ª–∞–¥", "–ö—Ä–µ–¥–∏—Ç", "–î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞"]
    
    def load_model(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ —Ñ–∞–π–ª–∞."""
        try:
            data = joblib.load(self.model_path)
            self.model = data["model"]
            self.scaler = data.get("scaler")
            self.products = data.get("products", ["–ò–ø–æ—Ç–µ–∫–∞", "–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", "–í–∫–ª–∞–¥", "–ö—Ä–µ–¥–∏—Ç"])
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
            self.scaler = StandardScaler()
            self.products = ["–ò–ø–æ—Ç–µ–∫–∞", "–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", "–í–∫–ª–∞–¥", "–ö—Ä–µ–¥–∏—Ç"]
    
    def save_model(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –≤ —Ñ–∞–π–ª."""
        Path(self.model_path).parent.mkdir(parents=True, exist_ok=True)
        
        data = {
            "model": self.model,
            "scaler": self.scaler,
            "products": self.products
        }
        
        joblib.dump(data, self.model_path)
    
    def train(
        self,
        X: List[List[float]],
        y: List[str],
        products: Optional[List[str]] = None
    ) -> None:
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å.
        
        :param X: –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        :param y: –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã
        :param products: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        """
        if products:
            self.products = products
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –≤ —á–∏—Å–ª–æ–≤—ã–µ
        product_to_idx = {p: i for i, p in enumerate(self.products)}
        y_numeric = [product_to_idx.get(label, 0) for label in y]
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_scaled = self.scaler.fit_transform(X)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        self.model.fit(X_scaled, y_numeric)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.save_model()
    
    def train_with_yandexgpt(
        self,
        user_profiles: List[Dict],
        use_synthetic: bool = True
    ) -> None:
        """
        –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é YandexGPT.
        
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ñ–∏–ª–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π,
        –∏—Å–ø–æ–ª—å–∑—É—è YandexGPT –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
        
        :param user_profiles: –°–ø–∏—Å–æ–∫ –ø—Ä–æ—Ñ–∏–ª–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        :param use_synthetic: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç YandexGPT
        """
        print("ü§ñ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å YandexGPT...")
        
        X_train = []
        y_train = []
        
        # –û–±—É—á–∞–µ–º –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª—è—Ö
        import time
        total_profiles = len(user_profiles)
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_profiles} –ø—Ä–æ—Ñ–∏–ª–µ–π...")
        
        for i, profile in enumerate(user_profiles[:50], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
            try:
                features = profile_to_features(profile)
                X_train.append(features)
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º YandexGPT –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
                print(f"  [{i}/{min(50, total_profiles)}] –ó–∞–ø—Ä–æ—Å –∫ YandexGPT –¥–ª—è –ø—Ä–æ—Ñ–∏–ª—è...")
                product = self._get_recommendation_from_yandexgpt(profile)
                y_train.append(product)
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è rate limiting
                if i < min(50, total_profiles):
                    time.sleep(0.5)
            except Exception as e:
                print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ—Ñ–∏–ª—è: {e}")
                continue
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if use_synthetic and len(user_profiles) > 0:
            print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ YandexGPT...")
            synthetic_data = self._generate_synthetic_training_data(user_profiles[:10])
            X_train.extend(synthetic_data["X"])
            y_train.extend(synthetic_data["y"])
        
        if len(X_train) > 0:
            print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(X_train)} –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
            self.train(X_train, y_train)
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é YandexGPT")
        else:
            print("‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ")
    
    def _get_recommendation_from_yandexgpt(self, profile: Dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –ø—Ä–æ–¥—É–∫—Ç–∞ –æ—Ç YandexGPT –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ñ–∏–ª—è.
        
        :param profile: –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        :return: –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è
        profile_text = f"""
–ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
- –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {profile.get('num_views', 0)}
- –ü–ª–∞—Ç–µ–∂–µ–π: {profile.get('num_payments', 0)}
- –°—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {profile.get('total_tx', 0)}
- –°—Ä–µ–¥–Ω–∏–π –ø–ª–∞—Ç–µ–∂: {profile.get('avg_tx', 0)}
- –î–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {profile.get('days_active', 0)}
- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {profile.get('unique_items', 0)}
- –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {profile.get('top_category', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
- –¢–æ–ø –±—Ä–µ–Ω–¥: {profile.get('top_brand', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
"""
        
        prompt = f"""{profile_text}

–û–ø—Ä–µ–¥–µ–ª–∏ –æ–¥–∏–Ω —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø—Ä–æ–¥—É–∫—Ç –ü–°–ë, –∫–æ—Ç–æ—Ä—ã–π –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç —ç—Ç–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã: –ò–ø–æ—Ç–µ–∫–∞, –ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞, –í–∫–ª–∞–¥, –ö—Ä–µ–¥–∏—Ç, –î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞.

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –ø—Ä–æ–¥—É–∫—Ç–∞, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π."""
        
        try:
            response = call_yandex_gpt(
                prompt,
                instructions="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–∞–Ω–∫–æ–≤—Å–∫–∏–º –ø—Ä–æ–¥—É–∫—Ç–∞–º. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–π –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø—Ä–æ–¥—É–∫—Ç.",
                temperature=0.3
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞
            response = response.strip()
            for product in self.products:
                if product.lower() in response.lower():
                    return product
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–æ–¥—É–∫—Ç
            return self.products[0]
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç YandexGPT: {e}")
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É
            if profile.get('total_tx', 0) > 100000:
                return "–í–∫–ª–∞–¥"
            elif profile.get('num_payments', 0) > 5:
                return "–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"
            else:
                return "–î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞"
    
    def _generate_synthetic_training_data(self, sample_profiles: List[Dict]) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ YandexGPT.
        
        :param sample_profiles: –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π
        :return: –°–ª–æ–≤–∞—Ä—å —Å X –∏ y –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        X_synthetic = []
        y_synthetic = []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª–µ–π
        for profile in sample_profiles[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
            try:
                # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª—è
                variations = [
                    {**profile, "num_payments": int(profile.get("num_payments", 0) * 1.5)},
                    {**profile, "total_tx": profile.get("total_tx", 0) * 2},
                    {**profile, "num_views": int(profile.get("num_views", 0) * 1.2)},
                ]
                
                for var_profile in variations:
                    features = profile_to_features(var_profile)
                    X_synthetic.append(features)
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –æ—Ç YandexGPT
                    product = self._get_recommendation_from_yandexgpt(var_profile)
                    y_synthetic.append(product)
            except Exception as e:
                print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
                continue
        
        return {"X": X_synthetic, "y": y_synthetic}
    
    def predict(
        self,
        user_profile: Dict,
        top_k: int = 3,
        graph: Optional[any] = None,
        patterns: Optional[List] = None
    ) -> List[Dict[str, any]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ø-K –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        
        :param user_profile: –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        :param top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        :param graph: –ì—Ä–∞—Ñ –ø–æ–≤–µ–¥–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ fallback)
        :param patterns: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–≤–µ–¥–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ fallback)
        :return: –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±—É—á–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å
        if self.model is None:
            return self._fallback_recommendations(user_profile, top_k, graph, patterns)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±—É—á–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å (–µ—Å—Ç—å –ª–∏ –∞—Ç—Ä–∏–±—É—Ç n_estimators_ –ø–æ—Å–ª–µ fit)
        if not hasattr(self.model, 'estimators_') or len(self.model.estimators_) == 0:
            print("‚ö† –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π fallback —Å –∞–Ω–∞–ª–∏–∑–æ–º –≥—Ä–∞—Ñ–∞ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
            return self._fallback_recommendations(user_profile, top_k, graph, patterns)
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–æ—Ñ–∏–ª—å –≤ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = profile_to_features(user_profile)
            X = np.array([features])
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ scaler –æ–±—É—á–µ–Ω
            if self.scaler and hasattr(self.scaler, 'mean_') and self.scaler.mean_ is not None:
                # Scaler –æ–±—É—á–µ–Ω (mean_ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Å–ª–µ fit)
                try:
                    X_scaled = self.scaler.transform(X)
                except Exception as e:
                    # –ï—Å–ª–∏ transform –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
                    print(f"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å scaler: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")
                    X_scaled = X
            else:
                # Scaler –Ω–µ –æ–±—É—á–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
                X_scaled = X
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
            scores = []
            for i, product in enumerate(self.products):
                # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—É—é –º–µ—Ç–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
                y_binary = np.zeros(len(self.products))
                y_binary[i] = 1
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                score = self.model.predict(X_scaled)[0]
                scores.append((product, float(score)))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ
            scores.sort(key=lambda x: x[1], reverse=True)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K
            recommendations = [
                {
                    "product": product,
                    "score": score
                }
                for product, score in scores[:top_k]
            ]
            
            return recommendations
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π fallback")
            return self._fallback_recommendations(user_profile, top_k, graph, patterns)
    
    def _fallback_recommendations(
        self,
        user_profile: Dict,
        top_k: int = 3,
        graph: Optional[any] = None,
        patterns: Optional[List] = None
    ) -> List[Dict[str, any]]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π fallback –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ñ–∏–ª—è, –≥—Ä–∞—Ñ–∞ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
        
        :param user_profile: –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        :param top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        :param graph: –ì—Ä–∞—Ñ –ø–æ–≤–µ–¥–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        :param patterns: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–≤–µ–¥–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        :return: –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        recommendations = []
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ—Ñ–∏–ª—è
        num_payments = user_profile.get("num_payments", 0)
        total_tx = user_profile.get("total_tx", 0)
        avg_tx = user_profile.get("avg_tx", 0)
        num_views = user_profile.get("num_views", 0)
        days_active = user_profile.get("days_active", 0)
        unique_items = user_profile.get("unique_items", 0)
        top_category = user_profile.get("top_category")
        top_brand = user_profile.get("top_brand")
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥—Ä–∞—Ñ–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        graph_scores = {}
        if graph is not None:
            try:
                import networkx as nx
                
                if graph.number_of_nodes() > 0:
                    # 1. PageRank –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —É–∑–ª–æ–≤
                    try:
                        pagerank = nx.pagerank(graph, max_iter=100, weight='weight')
                        
                        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã –≤–∞–∂–Ω—ã—Ö —É–∑–ª–æ–≤ –∏ –∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∏/–±—Ä–µ–Ω–¥—ã
                        item_nodes = []
                        brand_nodes = []
                        category_weights = {}
                        brand_weights = {}
                        
                        for node, data in graph.nodes(data=True):
                            node_importance = pagerank.get(node, 0)
                            if node_importance > 0.01:  # –¢–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ —É–∑–ª—ã
                                node_type = data.get("type", "unknown")
                                
                                if node_type == "item":
                                    item_nodes.append((node, node_importance))
                                    category_id = data.get("category_id")
                                    if category_id:
                                        category_weights[category_id] = category_weights.get(category_id, 0) + node_importance
                                
                                elif node_type == "brand":
                                    brand_nodes.append((node, node_importance))
                                    brand_id = data.get("brand_id")
                                    if brand_id:
                                        brand_weights[brand_id] = brand_weights.get(brand_id, 0) + node_importance
                        
                        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∏–ø–∞–º —É–∑–ª–æ–≤
                        total_item_importance = sum(imp for _, imp in item_nodes)
                        total_brand_importance = sum(imp for _, imp in brand_nodes)
                        
                        # –ï—Å–ª–∏ –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç –±—Ä–µ–Ω–¥—ã - –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏
                        if total_brand_importance > total_item_importance * 1.5:
                            graph_scores["–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"] = 0.4
                            graph_scores["–í–∫–ª–∞–¥"] = 0.25
                            if len(brand_nodes) > 5:
                                graph_scores["–í–∫–ª–∞–¥"] = 0.35  # –ú–Ω–æ–≥–æ —Ä–∞–∑–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤ = –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
                        
                        # –ï—Å–ª–∏ –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç —Ç–æ–≤–∞—Ä—ã - –∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä/–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
                        elif total_item_importance > total_brand_importance * 2:
                            graph_scores["–ò–ø–æ—Ç–µ–∫–∞"] = 0.3
                            graph_scores["–ö—Ä–µ–¥–∏—Ç"] = 0.25
                            if len(item_nodes) > 10:
                                graph_scores["–ò–ø–æ—Ç–µ–∫–∞"] = 0.4  # –ú–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ = –∫—Ä—É–ø–Ω–∞—è –ø–æ–∫—É–ø–∫–∞
                        
                        # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
                        if category_weights:
                            top_categories = sorted(category_weights.items(), key=lambda x: x[1], reverse=True)[:3]
                            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏/—Ä–µ–º–æ–Ω—Ç–∞ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –∏–ø–æ—Ç–µ–∫—É
                            for cat_id, weight in top_categories:
                                cat_str = str(cat_id).lower()
                                if any(keyword in cat_str for keyword in ["–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "—Ä–µ–º–æ–Ω—Ç", "–¥–æ–º", "–∫–≤–∞—Ä—Ç–∏—Ä–∞"]):
                                    graph_scores["–ò–ø–æ—Ç–µ–∫–∞"] = graph_scores.get("–ò–ø–æ—Ç–µ–∫–∞", 0) + 0.2 * weight
                                    break
                    except Exception as e:
                        print(f"‚ö† –û—à–∏–±–∫–∞ PageRank –∞–Ω–∞–ª–∏–∑–∞: {e}")
                    
                    # 2. –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞ (–ø—É—Ç–∏ –∏ —Å–≤—è–∑–Ω–æ—Å—Ç—å)
                    try:
                        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø—É—Ç–∏ –æ—Ç START
                        if "START" in graph:
                            reachable = list(nx.descendants(graph, "START"))
                            if reachable:
                                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª–∏–Ω—É –ø—É—Ç–µ–π
                                path_lengths = []
                                for target in reachable[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                                    try:
                                        paths = list(nx.all_simple_paths(graph, "START", target, cutoff=6))
                                        if paths:
                                            path_lengths.extend([len(p) for p in paths[:3]])
                                    except:
                                        continue
                                
                                if path_lengths:
                                    avg_path_length = sum(path_lengths) / len(path_lengths)
                                    max_path_length = max(path_lengths)
                                    
                                    # –î–ª–∏–Ω–Ω—ã–µ –ø—É—Ç–∏ = —Å–ª–æ–∂–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ = –∫—Ä—É–ø–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏
                                    if avg_path_length > 4 or max_path_length > 5:
                                        graph_scores["–ò–ø–æ—Ç–µ–∫–∞"] = graph_scores.get("–ò–ø–æ—Ç–µ–∫–∞", 0) + 0.25
                                        graph_scores["–ö—Ä–µ–¥–∏—Ç"] = graph_scores.get("–ö—Ä–µ–¥–∏—Ç", 0) + 0.2
                                    
                                    # –ú–Ω–æ–≥–æ –ø—É—Ç–µ–π = –∞–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
                                    if len(path_lengths) > 15:
                                        graph_scores["–ò–ø–æ—Ç–µ–∫–∞"] = graph_scores.get("–ò–ø–æ—Ç–µ–∫–∞", 0) + 0.15
                    except Exception as e:
                        print(f"‚ö† –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—É—Ç–µ–π: {e}")
                    
                    # 3. –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                    try:
                        density = nx.density(graph) if graph.number_of_nodes() > 1 else 0
                        
                        # –í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å = –∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
                        if density > 0.3:
                            graph_scores["–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"] = graph_scores.get("–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", 0) + 0.25
                            graph_scores["–î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞"] = graph_scores.get("–î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞", 0) + 0.2
                        
                        # –ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å, –Ω–æ –º–Ω–æ–≥–æ —É–∑–ª–æ–≤ = –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
                        elif density < 0.2 and graph.number_of_nodes() > 10:
                            graph_scores["–ò–ø–æ—Ç–µ–∫–∞"] = graph_scores.get("–ò–ø–æ—Ç–µ–∫–∞", 0) + 0.2
                            graph_scores["–ö—Ä–µ–¥–∏—Ç"] = graph_scores.get("–ö—Ä–µ–¥–∏—Ç", 0) + 0.15
                        
                        # –ê–Ω–∞–ª–∏–∑ —Å—Ç–µ–ø–µ–Ω–∏ —É–∑–ª–æ–≤ (—Å—Ä–µ–¥–Ω—è—è —Å—Ç–µ–ø–µ–Ω—å)
                        degrees = dict(graph.degree())
                        if degrees:
                            avg_degree = sum(degrees.values()) / len(degrees)
                            # –í—ã—Å–æ–∫–∞—è —Å—Ä–µ–¥–Ω—è—è —Å—Ç–µ–ø–µ–Ω—å = –∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
                            if avg_degree > 3:
                                graph_scores["–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"] = graph_scores.get("–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", 0) + 0.2
                    except Exception as e:
                        print(f"‚ö† –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏: {e}")
                    
                    # 4. –ê–Ω–∞–ª–∏–∑ –≤–µ—Å–æ–≤ —Ä—ë–±–µ—Ä (—á–∞—Å—Ç–æ—Ç—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π)
                    try:
                        edge_weights = [data.get("weight", 1) for _, _, data in graph.edges(data=True)]
                        if edge_weights:
                            avg_weight = sum(edge_weights) / len(edge_weights)
                            max_weight = max(edge_weights)
                            
                            # –í—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞ = —á–∞—Å—Ç—ã–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –¥–µ–π—Å—Ç–≤–∏—è
                            if avg_weight > 2 or max_weight > 5:
                                graph_scores["–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"] = graph_scores.get("–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", 0) + 0.15
                                graph_scores["–í–∫–ª–∞–¥"] = graph_scores.get("–í–∫–ª–∞–¥", 0) + 0.1
                    except Exception as e:
                        print(f"‚ö† –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ—Å–æ–≤: {e}")
                        
            except Exception as e:
                print(f"‚ö† –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≥—Ä–∞—Ñ–∞ –≤ fallback: {e}")
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
        pattern_scores = {}
        if patterns:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–º–æ–≥—É—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞–º–∏ –∏–ª–∏ –∫–æ—Ä—Ç–µ–∂–∞–º–∏)
            pattern_strings = []
            for p in patterns[:10]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –±–æ–ª—å—à–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
                if isinstance(p, tuple):
                    pattern_strings.append("‚Üí".join([str(x) for x in p]))
                elif isinstance(p, str):
                    pattern_strings.append(p)
            
            combined_pattern = " | ".join(pattern_strings)
            
            # 1. –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
            view_count = combined_pattern.count("V") + combined_pattern.count("view")
            pay_count = combined_pattern.count("P") + combined_pattern.count("pay")
            click_count = combined_pattern.count("C") + combined_pattern.count("click")
            total_events = view_count + pay_count + click_count
            
            if total_events > 0:
                view_ratio = view_count / total_events
                pay_ratio = pay_count / total_events
                
                # –î–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–µ–π
                if pay_ratio > 0.5:
                    pattern_scores["–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"] = 0.4
                    pattern_scores["–í–∫–ª–∞–¥"] = 0.3
                    if pay_count > 5:
                        pattern_scores["–í–∫–ª–∞–¥"] = 0.4  # –ú–Ω–æ–≥–æ –ø–ª–∞—Ç–µ–∂–µ–π = –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
                
                # –î–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
                elif view_ratio > 0.6:
                    pattern_scores["–ò–ø–æ—Ç–µ–∫–∞"] = 0.35
                    pattern_scores["–ö—Ä–µ–¥–∏—Ç"] = 0.25
                    if view_count > 10:
                        pattern_scores["–ò–ø–æ—Ç–µ–∫–∞"] = 0.45  # –ú–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ = –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
                
                # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
                elif 0.3 < view_ratio < 0.6 and 0.2 < pay_ratio < 0.5:
                    pattern_scores["–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"] = 0.3
                    pattern_scores["–ò–ø–æ—Ç–µ–∫–∞"] = 0.25
            
            # 2. –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (—Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
            for pattern_str in pattern_strings:
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: V‚ÜíV‚ÜíV –∏–ª–∏ V‚ÜíV‚ÜíP
                if "V‚ÜíV‚ÜíV" in pattern_str or pattern_str.count("V") >= 3:
                    pattern_scores["–ò–ø–æ—Ç–µ–∫–∞"] = pattern_scores.get("–ò–ø–æ—Ç–µ–∫–∞", 0) + 0.15
                    pattern_scores["–ö—Ä–µ–¥–∏—Ç"] = pattern_scores.get("–ö—Ä–µ–¥–∏—Ç", 0) + 0.1
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–∫—É–ø–æ–∫: P‚ÜíP‚ÜíP –∏–ª–∏ P‚ÜíP‚ÜíV
                if "P‚ÜíP‚ÜíP" in pattern_str or (pattern_str.count("P") >= 3 and pay_ratio > 0.5):
                    pattern_scores["–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"] = pattern_scores.get("–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", 0) + 0.2
                    pattern_scores["–í–∫–ª–∞–¥"] = pattern_scores.get("–í–∫–ª–∞–¥", 0) + 0.15
                
                # –°–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π: V‚ÜíP‚ÜíV –∏–ª–∏ P‚ÜíV‚ÜíP
                if "V‚ÜíP‚ÜíV" in pattern_str or "P‚ÜíV‚ÜíP" in pattern_str:
                    pattern_scores["–ò–ø–æ—Ç–µ–∫–∞"] = pattern_scores.get("–ò–ø–æ—Ç–µ–∫–∞", 0) + 0.2
                    pattern_scores["–ö—Ä–µ–¥–∏—Ç"] = pattern_scores.get("–ö—Ä–µ–¥–∏—Ç", 0) + 0.15
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –±—ã—Å—Ç—Ä—ã—Ö —Ä–µ—à–µ–Ω–∏–π: V‚ÜíP (–∫–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
                if len(pattern_str.split("‚Üí")) <= 3 and "V" in pattern_str and "P" in pattern_str:
                    pattern_scores["–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"] = pattern_scores.get("–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", 0) + 0.15
                    pattern_scores["–î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞"] = pattern_scores.get("–î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞", 0) + 0.1
            
            # 3. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            unique_patterns = len(set(pattern_strings))
            if unique_patterns > 5:
                # –ú–Ω–æ–≥–æ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ = —Å–ª–æ–∂–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ = –∫—Ä—É–ø–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏
                pattern_scores["–ò–ø–æ—Ç–µ–∫–∞"] = pattern_scores.get("–ò–ø–æ—Ç–µ–∫–∞", 0) + 0.1
                pattern_scores["–ö—Ä–µ–¥–∏—Ç"] = pattern_scores.get("–ö—Ä–µ–¥–∏—Ç", 0) + 0.1
        
        # –ë–∞–∑–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ñ–∏–ª—è
        base_scores = {}
        
        # –ò–ø–æ—Ç–µ–∫–∞ - –µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø–ª–∞—Ç–µ–∂–∏ –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏/—Ä–µ–º–æ–Ω—Ç–∞
        mortgage_score = 0.0
        if total_tx > 50000:  # –ö—Ä—É–ø–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏
            mortgage_score += 0.3
        if num_views > 10 and unique_items > 5:  # –ê–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä
            mortgage_score += 0.25
        if days_active > 7:  # –î–æ–ª–≥–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            mortgage_score += 0.2
        if top_category and ("–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å" in str(top_category).lower() or "—Ä–µ–º–æ–Ω—Ç" in str(top_category).lower()):
            mortgage_score += 0.25
        base_scores["–ò–ø–æ—Ç–µ–∫–∞"] = mortgage_score if mortgage_score > 0 else 0.1
        
        # –ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞ - –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏
        card_score = 0.0
        if num_payments > 5:  # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏
            card_score += 0.4
        if avg_tx > 1000 and avg_tx < 50000:  # –°—Ä–µ–¥–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–∏
            card_score += 0.3
        if days_active > 3:  # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            card_score += 0.2
        base_scores["–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"] = card_score if card_score > 0 else 0.2
        
        # –í–∫–ª–∞–¥ - –µ—Å–ª–∏ –µ—Å—Ç—å –∫—Ä—É–ø–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏
        deposit_score = 0.0
        if total_tx > 100000:  # –ö—Ä—É–ø–Ω—ã–µ —Å—É–º–º—ã
            deposit_score += 0.5
        if num_payments > 10:  # –ú–Ω–æ–≥–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
            deposit_score += 0.2
        if avg_tx > 10000:  # –ö—Ä—É–ø–Ω—ã–µ —Å—Ä–µ–¥–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–∏
            deposit_score += 0.2
        base_scores["–í–∫–ª–∞–¥"] = deposit_score if deposit_score > 0 else 0.15
        
        # –ö—Ä–µ–¥–∏—Ç - –µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã
        loan_score = 0.0
        if num_views > 15:  # –ú–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
            loan_score += 0.4
        if unique_items > 10:  # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤
            loan_score += 0.3
        if days_active > 5:  # –î–æ–ª–≥–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            loan_score += 0.2
        base_scores["–ö—Ä–µ–¥–∏—Ç"] = loan_score if loan_score > 0 else 0.1
        
        # –î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ - –±–∞–∑–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        base_scores["–î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞"] = 0.25 if (num_payments > 0 or num_views > 0) else 0.3
        
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        final_scores = {}
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç—É–ø–Ω—ã
        has_graph = graph is not None and graph.number_of_nodes() > 0
        has_patterns = patterns and len(patterns) > 0
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
        if has_graph and has_patterns:
            # –í—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã - —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞
            base_weight, graph_weight, pattern_weight = 0.4, 0.35, 0.25
        elif has_graph:
            # –¢–æ–ª—å–∫–æ –≥—Ä–∞—Ñ - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –µ–≥–æ –≤–µ—Å
            base_weight, graph_weight, pattern_weight = 0.5, 0.5, 0.0
        elif has_patterns:
            # –¢–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω—ã - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏—Ö –≤–µ—Å
            base_weight, graph_weight, pattern_weight = 0.6, 0.0, 0.4
        else:
            # –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            base_weight, graph_weight, pattern_weight = 1.0, 0.0, 0.0
        
        for product in self.products:
            final_scores[product] = (
                base_scores.get(product, 0) * base_weight +
                graph_scores.get(product, 0) * graph_weight +
                pattern_scores.get(product, 0) * pattern_weight
            )
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º, —á—Ç–æ–±—ã –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –±—ã–ª–∞ 1.0
            if final_scores[product] > 1.0:
                final_scores[product] = 1.0
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ
        sorted_products = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K
        for product, score in sorted_products[:top_k]:
            recommendations.append({
                "product": product,
                "score": float(score)
            })
        
        return recommendations


def recommend(
    user_profile: Dict,
    model_path: Optional[str] = None,
    top_k: int = 3,
    graph: Optional[any] = None,
    patterns: Optional[List] = None
) -> List[Dict[str, any]]:
    """
    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –ø—Ä–æ–¥—É–∫—Ç—ã –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    
    :param user_profile: –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    :param model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    :param top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    :param graph: –ì—Ä–∞—Ñ –ø–æ–≤–µ–¥–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ fallback)
    :param patterns: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–≤–µ–¥–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ fallback)
    :return: –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    """
    model = NBOModel(model_path=model_path)
    return model.predict(user_profile, top_k=top_k, graph=graph, patterns=patterns)

